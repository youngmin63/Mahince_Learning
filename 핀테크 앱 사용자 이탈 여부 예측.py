# -*- coding: utf-8 -*-
"""2churn_Template_with_Guide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y_U-K8OHANIrbQlOQ6rYjlsAf34DYBF7

# ğŸ“‰ í•€í…Œí¬ ì‚¬ìš©ì ì´íƒˆ ì˜ˆì¸¡ í…œí”Œë¦¿
ëª©ì : ì‚¬ìš©ì í–‰ë™ ë° ì„œë¹„ìŠ¤ ì´ìš© ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•€í…Œí¬ ì•± ì‚¬ìš©ì ì´íƒˆ(user_churn) ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•

###  í•€í…Œí¬ ì•± ì‚¬ìš©ì ì´íƒˆ ì˜ˆì¸¡

ğŸ“Œ

#### 1. ë°ì´í„° ì „ì²˜ë¦¬
- **ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°**  
  - `user_id`ëŠ” ì˜ˆì¸¡ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°
- **ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©**  
  - `preferred_channel`, `membership_type`, `preferred_payment_type`, `app_security_enabled`, `in_app_support_use` ë“±
- **ì—°ì†í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§**  
  - `months_active`, `monthly_spending`, `total_transaction_amt` ë“±ì— `StandardScaler` ë˜ëŠ” `MinMaxScaler` ì ìš©
- **Xì™€ y ë¶„í• **  
  - íƒ€ê¹ƒ ë³€ìˆ˜ `user_churn` (0: ìœ ì§€, 1: ì´íƒˆ), ë‚˜ë¨¸ì§€ë¥¼ Xë¡œ ì„¤ì •

#### 2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
- ì´íƒˆ ê³ ê°ê³¼ ìœ ì§€ ê³ ê° ê°„ ì£¼ìš” ë³€ìˆ˜ ì°¨ì´ ì‹œê°í™”  
  - ì˜ˆ: `monthly_spending`, `months_active`, `membership_type`ë³„ ì´íƒˆë¥  ë¹„êµ
- ì´íƒˆ ë¹„ìœ¨, ë¶„í¬ ë¶ˆê· í˜• ì—¬ë¶€ í™•ì¸

#### 3. ëª¨ë¸ë§ ë° ì„±ëŠ¥ í‰ê°€
- 2ê°€ì§€ ì´ìƒ ë¶„ë¥˜ ëª¨ë¸ ì ìš©  
  - ì˜ˆ: `LogisticRegression`, `RandomForestClassifier` ë“±
- `train_test_split` í›„ ëª¨ë¸ í•™ìŠµ
- ì •í™•ë„(accuracy), ì¬í˜„ìœ¨(recall), ì •ë°€ë„(precision), F1-score ë“± í‰ê°€ ì§€í‘œ ì¶œë ¥
- **Confusion Matrix**ì™€ **ROC Curve** ì‹œê°í™”

#### 4. ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
- ëª¨ë¸ ê³„ìˆ˜ ë˜ëŠ” feature importanceë¥¼ ì‹œê°í™”
- `membership_type`, `preferred_channel`, `spending` ë“±ì´ ì´íƒˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í•´ì„

#### 5. ê²°ê³¼ í•´ì„
- ê³ ê° ì´íƒˆì„ ì˜ˆì¸¡í•˜ëŠ” ë° ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€?
- í•€í…Œí¬ ì•± ì‚¬ìš©ì ì´íƒˆì„ ì¤„ì´ê¸° ìœ„í•œ ì‹œì‚¬ì  ì œì‹œ

### âœ… ë³€ìˆ˜ ì„¤ëª… (í•€í…Œí¬ ì‚¬ìš©ì ì´íƒˆ ì˜ˆì¸¡ ë°ì´í„°)

| ë³€ìˆ˜ëª… | ì„¤ëª… |
|--------|------|
| `user_id` | ì‚¬ìš©ì ê³ ìœ  ID |
| `months_active` | í•€í…Œí¬ ì•±ì„ ì‚¬ìš©í•œ ëˆ„ì  ê°œì›” ìˆ˜ |
| `monthly_spending` | ì›” í‰ê·  ê²°ì œ ê¸ˆì•¡ |
| `total_transaction_amt` | ì „ì²´ ëˆ„ì  ê²°ì œ ê¸ˆì•¡ |
| `preferred_channel` | ì£¼ ì´ìš© ì±„ë„ (ì˜ˆ: mobile, web) |
| `membership_type` | ë©¤ë²„ì‹­ ìœ í˜• (free, standard, premium) |
| `preferred_payment_type` | ì„ í˜¸ ê²°ì œ ë°©ì‹ (card, simple_pay, auto_transfer ë“±) |
| `app_security_enabled` | ë³´ì•ˆ ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€ (ì˜ˆ: ìƒì²´ì¸ì¦ ë“±) |
| `in_app_support_use` | ì•± ë‚´ ê³ ê°ì§€ì› ê¸°ëŠ¥ ì´ìš© ì—¬ë¶€ |
| `user_churn` | ì‚¬ìš©ì ì´íƒˆ ì—¬ë¶€ (1: ì´íƒˆ, 0: ìœ ì§€) |
"""

!pip install lightgbm

# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb

# ğŸ“¥ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('fintech_user_churn.csv')
df.head()

"""**ì´íƒˆì ë¹„ìœ¨ í™•ì¸**"""

# user_churn ë¶„í¬ ì‹œê°í™”
sns.countplot(data=df, x='user_churn', palette='Set2')
plt.title("ğŸ” ì‚¬ìš©ì ì´íƒˆ ì—¬ë¶€ ë¶„í¬")
plt.xlabel("user_churn (0=ìœ ì§€, 1=ì´íƒˆ)")
plt.ylabel("ê³ ê° ìˆ˜")
plt.show()

# ë¹„ìœ¨ ì¶œë ¥
churn_rate = df['user_churn'].value_counts(normalize=True)
churn_rate

"""**ì—°ì†í˜• ë³€ìˆ˜ ë¶„í¬ ë¹„êµ**"""

sns.boxplot(data=df, x='user_churn', y='monthly_spending', palette='Set3')
plt.title("ğŸ’¸ ì›” í‰ê·  ì§€ì¶œ (monthly_spending) vs ì´íƒˆ ì—¬ë¶€")
plt.xlabel("user_churn (0=ìœ ì§€, 1=ì´íƒˆ)")
plt.ylabel("monthly_spending")
plt.show()

sns.boxplot(data=df, x='user_churn', y='months_active', palette='Set3')
plt.title("â³ ì‚¬ìš© ê°œì›” ìˆ˜ (months_active) vs ì´íƒˆ ì—¬ë¶€")
plt.xlabel("user_churn (0=ìœ ì§€, 1=ì´íƒˆ)")
plt.ylabel("months_active")
plt.show()

"""ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ë¹„êµ"""

membership_churn = pd.crosstab(df['membership_type'], df['user_churn'], normalize='index')
membership_churn.plot(kind='bar', stacked=True, colormap='Pastel1')
plt.title("ğŸ« ë©¤ë²„ì‹­ ìœ í˜•ë³„ ì´íƒˆë¥ ")
plt.ylabel("ë¹„ìœ¨")
plt.legend(title='user_churn', labels=['ìœ ì§€(0)', 'ì´íƒˆ(1)'])
plt.show()

# ğŸ§¹ ì „ì²˜ë¦¬: ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ë° ë¶„í• 
df_encoded = pd.get_dummies(df.drop('user_id', axis=1), drop_first=True)
X = df_encoded.drop('user_churn', axis=1)
y = df_encoded['user_churn']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# ğŸ¤– ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# ğŸ“Š í‰ê°€ ê²°ê³¼ ì¶œë ¥
print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""í´ë˜ìŠ¤1ì„ ì•„ì˜ˆ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ì—¬ RandomForest ëª¨ë¸ ì ìš©"""

model=RandomForestClassifier(random_state=42)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ìœ„í•´ class_weight= balanced ì ìš©"""

model=RandomForestClassifier(class_weight='balanced',random_state=42)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""SMOTHE ì˜¤ë²„ìƒ˜í”Œë§ ì ìš© : í¬ê·€ í´ë˜ìŠ¤ ìƒ˜í”Œ ëŠ˜ë¦¬ê¸°
-> recall ê°œì„ ë¨ : ì‹¤ì œ ì´íƒˆìë¥¼ ë” ì‹ ê²½ì“°ê¸° ì‹œì‘í•¨
"""

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

model = RandomForestClassifier(random_state=42)
model.fit(X_resampled, y_resampled)
y_pred= model.predict(X_test)

print(classification_report(y_test, y_pred))

# 1. ì˜¤ë²„ìƒ˜í”Œë§ (SMOTE)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 4],
    'class_weight': ['balanced']
}

# 3. GridSearchCV ì ìš©
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_resampled, y_resampled)

# 4. ìµœì  ëª¨ë¸ ì¶”ì¶œ ë° ì˜ˆì¸¡
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

"""í´ë˜ìŠ¤ 1ì˜ˆì¸¡ ì—¬ì „íˆ ë¶€ì¡±í•¨ -> ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ë§Œìœ¼ë¡œëŠ” í•œê³„

LightGBM ì ìš©
"""

# SMOTE ì˜¤ë²„ìƒ˜í”Œë§
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# LightGBM ë°ì´í„°ì…‹ êµ¬ì„±
lgb_train = lgb.Dataset(X_resampled, label=y_resampled)
lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt',
    'is_unbalance': True,  # âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘
    'learning_rate': 0.05,
    'num_leaves': 31,
    'random_state': 42
}

# ëª¨ë¸ í•™ìŠµ
model = lgb.train(params, lgb_train, num_boost_round=100)

# ì˜ˆì¸¡
y_proba = model.predict(X_test)
y_pred = (y_proba > 0.3).astype(int)

print(classification_report(y_test, y_pred))

"""**ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”**"""

feature_imp = pd.Series(model.feature_importance(), index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(8, 6))
feature_imp.head(10).plot(kind='barh')
plt.title("ğŸ” Top 10 ì¤‘ìš”í•œ ë³€ìˆ˜ (LightGBM ê¸°ì¤€)")
plt.xlabel("ì¤‘ìš”ë„ ì ìˆ˜")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""## ğŸ“Œ ê²°ê³¼ í•´ì„
ëª¨ë¸ ë¶„ì„ ê²°ê³¼, total_transaction_amt, months_active, monthly_spending ë“± ê³ ê°ì˜ ì´ìš© ì¶©ì„±ë„ ë° ì†Œë¹„ íŒ¨í„´ì„ ë°˜ì˜í•˜ëŠ” ë³€ìˆ˜ë“¤ì´ ì´íƒˆ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì³¤ë‹¤. ë˜í•œ membership_typeì´ë‚˜ app_security_enabled ê°™ì€ ì„¤ì • ê´€ë ¨ ë³€ìˆ˜ë“¤ë„ ì¤‘ìš”í•˜ê²Œ ì‘ìš©í•˜ì—¬, ë‹¨ìˆœ í–‰ë™ ì™¸ì—ë„ ì‚¬ìš©ì ìœ í˜•ê³¼ ìŠµê´€ì´ ì´íƒˆê³¼ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.
"""