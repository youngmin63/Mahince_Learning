# -*- coding: utf-8 -*-
"""2churn_Template_with_Guide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y_U-K8OHANIrbQlOQ6rYjlsAf34DYBF7

# 📉 핀테크 사용자 이탈 예측 템플릿
목적: 사용자 행동 및 서비스 이용 정보를 바탕으로 핀테크 앱 사용자 이탈(user_churn) 여부를 예측하는 분류 모델을 구축

###  핀테크 앱 사용자 이탈 예측

📌

#### 1. 데이터 전처리
- **불필요한 컬럼 제거**  
  - `user_id`는 예측에 불필요하므로 제거
- **범주형 변수 인코딩**  
  - `preferred_channel`, `membership_type`, `preferred_payment_type`, `app_security_enabled`, `in_app_support_use` 등
- **연속형 변수 스케일링**  
  - `months_active`, `monthly_spending`, `total_transaction_amt` 등에 `StandardScaler` 또는 `MinMaxScaler` 적용
- **X와 y 분할**  
  - 타깃 변수 `user_churn` (0: 유지, 1: 이탈), 나머지를 X로 설정

#### 2. 탐색적 데이터 분석 (EDA)
- 이탈 고객과 유지 고객 간 주요 변수 차이 시각화  
  - 예: `monthly_spending`, `months_active`, `membership_type`별 이탈률 비교
- 이탈 비율, 분포 불균형 여부 확인

#### 3. 모델링 및 성능 평가
- 2가지 이상 분류 모델 적용  
  - 예: `LogisticRegression`, `RandomForestClassifier` 등
- `train_test_split` 후 모델 학습
- 정확도(accuracy), 재현율(recall), 정밀도(precision), F1-score 등 평가 지표 출력
- **Confusion Matrix**와 **ROC Curve** 시각화

#### 4. 변수 중요도 분석
- 모델 계수 또는 feature importance를 시각화
- `membership_type`, `preferred_channel`, `spending` 등이 이탈에 미치는 영향 해석

#### 5. 결과 해석
- 고객 이탈을 예측하는 데 가장 중요한 변수는 무엇인가?
- 핀테크 앱 사용자 이탈을 줄이기 위한 시사점 제시

### ✅ 변수 설명 (핀테크 사용자 이탈 예측 데이터)

| 변수명 | 설명 |
|--------|------|
| `user_id` | 사용자 고유 ID |
| `months_active` | 핀테크 앱을 사용한 누적 개월 수 |
| `monthly_spending` | 월 평균 결제 금액 |
| `total_transaction_amt` | 전체 누적 결제 금액 |
| `preferred_channel` | 주 이용 채널 (예: mobile, web) |
| `membership_type` | 멤버십 유형 (free, standard, premium) |
| `preferred_payment_type` | 선호 결제 방식 (card, simple_pay, auto_transfer 등) |
| `app_security_enabled` | 보안 기능 사용 여부 (예: 생체인증 등) |
| `in_app_support_use` | 앱 내 고객지원 기능 이용 여부 |
| `user_churn` | 사용자 이탈 여부 (1: 이탈, 0: 유지) |
"""

!pip install lightgbm

# 📦 라이브러리 불러오기
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb

# 📥 데이터 불러오기
df = pd.read_csv('fintech_user_churn.csv')
df.head()

"""**이탈자 비율 확인**"""

# user_churn 분포 시각화
sns.countplot(data=df, x='user_churn', palette='Set2')
plt.title("🔍 사용자 이탈 여부 분포")
plt.xlabel("user_churn (0=유지, 1=이탈)")
plt.ylabel("고객 수")
plt.show()

# 비율 출력
churn_rate = df['user_churn'].value_counts(normalize=True)
churn_rate

"""**연속형 변수 분포 비교**"""

sns.boxplot(data=df, x='user_churn', y='monthly_spending', palette='Set3')
plt.title("💸 월 평균 지출 (monthly_spending) vs 이탈 여부")
plt.xlabel("user_churn (0=유지, 1=이탈)")
plt.ylabel("monthly_spending")
plt.show()

sns.boxplot(data=df, x='user_churn', y='months_active', palette='Set3')
plt.title("⏳ 사용 개월 수 (months_active) vs 이탈 여부")
plt.xlabel("user_churn (0=유지, 1=이탈)")
plt.ylabel("months_active")
plt.show()

"""범주형 변수 분포 비교"""

membership_churn = pd.crosstab(df['membership_type'], df['user_churn'], normalize='index')
membership_churn.plot(kind='bar', stacked=True, colormap='Pastel1')
plt.title("🎫 멤버십 유형별 이탈률")
plt.ylabel("비율")
plt.legend(title='user_churn', labels=['유지(0)', '이탈(1)'])
plt.show()

# 🧹 전처리: 범주형 변수 인코딩 및 분할
df_encoded = pd.get_dummies(df.drop('user_id', axis=1), drop_first=True)
X = df_encoded.drop('user_churn', axis=1)
y = df_encoded['user_churn']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# 🤖 로지스틱 회귀 모델 학습 및 예측
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 📊 평가 결과 출력
print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""클래스1을 아예 예측하지 못하여 RandomForest 모델 적용"""

model=RandomForestClassifier(random_state=42)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""클래스 불균형 해결위해 class_weight= balanced 적용"""

model=RandomForestClassifier(class_weight='balanced',random_state=42)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""SMOTHE 오버샘플링 적용 : 희귀 클래스 샘플 늘리기
-> recall 개선됨 : 실제 이탈자를 더 신경쓰기 시작함
"""

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

model = RandomForestClassifier(random_state=42)
model.fit(X_resampled, y_resampled)
y_pred= model.predict(X_test)

print(classification_report(y_test, y_pred))

# 1. 오버샘플링 (SMOTE)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 2. 하이퍼파라미터 그리드 설정
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 4],
    'class_weight': ['balanced']
}

# 3. GridSearchCV 적용
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_resampled, y_resampled)

# 4. 최적 모델 추출 및 예측
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

"""클래스 1예측 여전히 부족함 -> 랜덤 포레스트 모델 만으로는 한계

LightGBM 적용
"""

# SMOTE 오버샘플링
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# LightGBM 데이터셋 구성
lgb_train = lgb.Dataset(X_resampled, label=y_resampled)
lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)

# 하이퍼파라미터 설정
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt',
    'is_unbalance': True,  # ⚠️ 클래스 불균형 대응
    'learning_rate': 0.05,
    'num_leaves': 31,
    'random_state': 42
}

# 모델 학습
model = lgb.train(params, lgb_train, num_boost_round=100)

# 예측
y_proba = model.predict(X_test)
y_pred = (y_proba > 0.3).astype(int)

print(classification_report(y_test, y_pred))

"""**변수 중요도 시각화**"""

feature_imp = pd.Series(model.feature_importance(), index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(8, 6))
feature_imp.head(10).plot(kind='barh')
plt.title("🔍 Top 10 중요한 변수 (LightGBM 기준)")
plt.xlabel("중요도 점수")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""## 📌 결과 해석
모델 분석 결과, total_transaction_amt, months_active, monthly_spending 등 고객의 이용 충성도 및 소비 패턴을 반영하는 변수들이 이탈 예측에 가장 큰 영향을 미쳤다. 또한 membership_type이나 app_security_enabled 같은 설정 관련 변수들도 중요하게 작용하여, 단순 행동 외에도 사용자 유형과 습관이 이탈과 유의미한 관계가 있음을 확인할 수 있었다.
"""